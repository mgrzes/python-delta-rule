{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exploration of the Delta Rule\n",
    "University of Kent\n",
    "\n",
    "CO636/836\n",
    "\n",
    "Marek Grzes\n",
    "\n",
    "Last modified 14/10/2020\n",
    "\n",
    "This exploration complements Sec. 5.3 in O'reilly & Munakata (2000) that introduces the delta rule for the Summed Squared Error (SSE) and linear activation. We apply the delta rule to a tiny classification problem with two classes. Since we are interested in classification, the delta rule with a sigmoidal activation (i.e. logistic regression) would be more appropriate for this problem in general, but we start with the linear activation for didactic reasons. The implications of using sigmoidal activation in the delta rule are presented in Sec. 5.4 in the textbook, and we use this fact in the final questions that are at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate Data\n",
    "\n",
    "We generate a small dataset with two variables $x$ and $y$, and a binary class\n",
    "attribute with values $-1$ and $1$. The data is generated from two normal\n",
    "distributions in which the variables $x$ and $y$ are not uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the number of data points per class\n",
    "num_points = 20\n",
    "\n",
    "# data points for class -1\n",
    "data1 = np.random.multivariate_normal([1,1], [[0.8, 0.6],[0.6, 0.7]], num_points)\n",
    "# data points for class 1\n",
    "data2 = np.random.multivariate_normal([1,-1], [[0.8, 0.6],[0.6, 0.7]], num_points)\n",
    "\n",
    "# plot both arrays\n",
    "# note that arrays are indexed [row, col]\n",
    "plt.scatter(data1[:,0], data1[:,1], c='green')\n",
    "plt.scatter(data2[:,0], data2[:,1], c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we add the class attribute in the third column, and we combine the two parts into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we create one dataset with the class attribute\n",
    "# 1) add one column with the class label\n",
    "# create the class labels for both classes\n",
    "c0 = - np.ones([num_points, 1])\n",
    "c1 = np.ones([num_points, 1])\n",
    "# append the class labels to both datasets\n",
    "data1 = np.append(data1, c0, axis=1)\n",
    "data2 = np.append(data2, c1, axis=1)\n",
    "# 2) combine the two classes in one data array\n",
    "data = np.append(data1, data2, axis=0)\n",
    "\n",
    "# we shuffle the data by rows because we want positive and negative examples to be interleaved; the algorithm below will update the weights after processing one data example\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# print the first 5 rows to see what the data looks like\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We plot the final dataset that contains both classes in one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=data[:,2] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Delta Rule\n",
    "\n",
    "We will use the delta rule algorithm to solve our classification problem. Variables\n",
    "$t_k$ and $o_k$ are used to be consistent with the textbook (O'reilly & Munakata 2000).\n",
    "\n",
    "The code that is below uses the delta rule update equation: $\\Delta w_{i,k}=\\epsilon(t_k-o_k)s_i$ that is presented in Eq. (5.3) in O'reilly & Munakata (2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A coroutine that receives data and prints it\n",
    "def printer():\n",
    "    i = 0\n",
    "    line=\"\"\n",
    "    while True:\n",
    "        vec = yield\n",
    "        if vec.size == 1:\n",
    "            if i > 0:\n",
    "                # print anything that is left in the buffer\n",
    "                print(line)\n",
    "                i = 0\n",
    "                line = \"\"\n",
    "        else:\n",
    "            i = i + 1\n",
    "            line = line + \" \" + str(np.round(vec,4))\n",
    "            if i % 3 == 0:\n",
    "                print(line)\n",
    "                line = \"\"\n",
    "                i = 0\n",
    "\n",
    "printer_handler = printer()\n",
    "# start the printer\n",
    "printer_handler.send(None)\n",
    "\n",
    "# we need 3 parameters: one for x, one for y, and one for the bias term\n",
    "w = np.random.normal(0, 0.5, 3)\n",
    "# learning rate\n",
    "epsilon = 0.05\n",
    "delta = 100\n",
    "iteration = 0\n",
    "precision=0.01\n",
    "\n",
    "while delta > precision and iteration < 10:\n",
    "    iteration = iteration + 1\n",
    "    # to be consistent with the textbook, we will process one data point at a time\n",
    "    for s in data:\n",
    "        # compute the linear activation (predicted output)\n",
    "        s_with_bias = np.copy(s)\n",
    "        s_with_bias[2] = 1 # the last element is used as the bias activation\n",
    "        o_k =  w.dot(s_with_bias) # see Eq. 5.6 in O'reilly & Munakata (2000)\n",
    "        t_k = s[2]\n",
    "        # the delta rule equation in the vector form\n",
    "        delta_w = epsilon * (t_k - o_k) * s_with_bias # see Eq. 5.3\n",
    "        w = w + delta_w\n",
    "        # if the max change is tiny, we will stop learning in the while loop\n",
    "        delta=np.max(np.absolute(delta_w))\n",
    "        # print the current weights\n",
    "        printer_handler.send(w)\n",
    "\n",
    "# print the remaining values if there are any\n",
    "printer_handler.send(np.zeros([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Final Decision Boundary and the Threshold\n",
    "\n",
    "Our class attribute has values -1 and 1 whereas the outputs of the linear model are between minus and plus infinity. This means that we need to introduce a threshold that will allow us to map the continuous output to the class value. A threshold of zero is a good default choice because it lies between -1 and 1. With that, all\n",
    "activations that are negative will be mapped to class -1, and all activations that are positive or zero will be mapped to class +1.\n",
    "\n",
    "Our decision boundary in 3D is $z=w[0]\\times x + w[1]\\times y + w[2]$. When the threshold is zero, this boundary in the 2D space defined by the xy-plane becomes $y = -\\frac{w[0]}{w[1]}\\times x-\\frac{w[2]}{w[1]}$. This will be the red line in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the final line\n",
    "\n",
    "# construct the line using 100 points\n",
    "x = np.linspace(np.min(data[:,0]),np.max(data[:,0]),100)\n",
    "# this equation corresponds to our decision boundary on the xy-plane when the threshold is zero\n",
    "y = -w[0]/w[1]*x-w[2]/w[1]\n",
    "ymin = np.min(data[:,1])\n",
    "ymax = np.max(data[:,1])\n",
    "\n",
    "# plot the data and the line\n",
    "plt.ylim(ymin-0.3, ymax+0.3)\n",
    "# plot two classes separately\n",
    "plt.scatter(data1[:,0], data1[:,1], c='green', label='Class -1' )\n",
    "plt.scatter(data2[:,0], data2[:,1], c='blue', label='Class 1' )\n",
    "plt.plot(x, y, '-r', label='boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Error on Training Data\n",
    "\n",
    "We will compute the number of errors on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to add bias activations in the last column\n",
    "data_with_bias = np.copy(data)\n",
    "data_with_bias[:,2]=1\n",
    "\n",
    "# predictions are the outputs computed by the linear model\n",
    "predictions = w.dot(data_with_bias.transpose())\n",
    "\n",
    "# compute the summed squared error with respect to the linear activation; i.e., we want to see see how well the linear function approximates the values of our class attribute\n",
    "SSE = np.sum(np.square(predictions-data[:,2]))\n",
    "\n",
    "# this is the same threshold that we used to derive the red decision boundary above; with this threshold, our classification below will be consistent with the red line above\n",
    "threshold = 0\n",
    "\n",
    "# apply threshold to make discrete decisions\n",
    "predictions[predictions<threshold] = -1\n",
    "predictions[predictions>=threshold] = 1\n",
    "# print(predictions)\n",
    "# print(data[:,2])\n",
    "\n",
    "# diff will be zero when the classification is correct\n",
    "diff = predictions - data[:,2]\n",
    "# indices of data items with incorrect predictions\n",
    "erroneous = diff.nonzero()[0]\n",
    "# count the number of incorrect predictions\n",
    "accuracy = np.size(erroneous)\n",
    "\n",
    "print(\"SSE on training data: \" + str(SSE) )\n",
    "print(\"Number of errors on training data: \" + str(accuracy) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions\n",
    "\n",
    "You are not required to answer all the questions that are below. I would encourage you to attempt the first five. The last two questions require a certain amount of coding in Python, and they can be skipped by those who are not interested in further exploration.\n",
    "\n",
    "1. Read this notebook and analyse all sections trying to understand them and trying to see the link with the textbook and the lectures.\n",
    "\n",
    "2. The red line in Sec. _\"The Final Decision Boundary and the Threshold\"_ is the decision boundary for our classification problem, and it corresponds to the threshold of zero. How would this decision boundary change if\n",
    "the threshold was smaller or greater than zero? Note which data points have the class -1 and which +1; this can be seen in the legend in the figure. You may want to do subsequent questions first if this question is challenging for you initially.\n",
    "\n",
    "2. Add computation of the Summed Squared Error (SSE) in the \"while\" loop of the learning algorithm and print its value. Note that we are interested in the SSE between the true class value and the linear output of our neural network because this is what the delta rule optimises.\n",
    "\n",
    "3. In the current implementation, the final decision boundary (the red line above)\n",
    " is plotted after the learning process has finished. Your task is to plot the\n",
    " data and the current boundary in the \"while\" or \"for\" loop of the learning algorithm. The goal is to investigate how the decision boundary is changing during learning, and to contrast those changes with the changes in the SSE. You basically want to see the decision boundary after every application of the delta rule equation.\n",
    "\n",
    "4. If you run the simulation many times, you will notice that the SSE may be non-zero even if the number of classification errors on the training data is zero. Explain how this is possible.\n",
    "\n",
    "5. The decision boundary that is learned has the form: $z=w[0]\\times x+w[1]\\times y+w[2]$ where $z$ is the last column in our training data, and it represents the class attribute.  Similarly, $x$ and $y$ are the first and the second columns in the data array respectively. This means that strictly speaking the decision boundary is a plane located in a 3-dimensional xyz-space. Explain how this plane is placed with respect to the flat scatterplot of the data examples in their xy-plane (i.e. the graph with the red line). After trying to estimate the location of the separating hyperplane in 3D, you could construct a 3-dimensional version of the this plot to see where exactly this plane is located. An example 3D scatterplot in mathplotib is here https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html\n",
    "\n",
    "6. Add sigmoidal activation to this implementation. Note that assuming that we are switching the Cross-Entropy error (CE) instead of the Summed Squared Error (SSE), the update equation for the delta rule remains unchanged. The only difference is the activation function that is sigmoidal and takes the original linear activation as input. This is well explained in Sec. 5.4 in O'reilly & Munakata (2000). Since the range of the sigmoid function is the interval $[0,1]$, it would make sense to replace the class -1 with 0 in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "O'reilly, Randall C., and Yuko Munakata. Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain. MIT press, 2000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<small>This document was written in Pycharm Professional and compiled using Python 3.8 with standard packages available in Ubuntu 20.04.</small>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (python-delta-rule)",
   "language": "python",
   "name": "pycharm-eef80f61"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
